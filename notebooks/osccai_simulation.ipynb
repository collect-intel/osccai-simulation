{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OSCCAI Simulation Notebook\n",
    "\n",
    "## Usage Instructions\n",
    "\n",
    "This notebook simulates output from an Open Source Collective Constitutional AI (OSCCAI) tool.\n",
    "Please ensure you have the necessary libraries installed as specified in the `requirements.txt` file.\n",
    "You will need an OpenAI API key to run this notebook. Set it as an environment variable or enter it when prompted.\n",
    "\n",
    "**Required Setup:**\n",
    "- Install required libraries using `pip install -r requirements.txt`\n",
    "- Set your OpenAI API key:\n",
    "  - Option 1: Set as an environment variable `OPENAI_API_KEY`\n",
    "  - Option 2: Enter it when prompted in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell to install required packages\n",
    "!pip install numpy pandas matplotlib openai tenacity asyncio nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from getpass import getpass\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import beta, nbinom\n",
    "import random\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "\n",
    "# Try to import from google.colab if available, otherwise use a fallback\n",
    "try:\n",
    "    from google.colab import files, userdata\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    \n",
    "    # Fallback function for file upload in Jupyter Notebook\n",
    "    def upload_files():\n",
    "        from ipywidgets import FileUpload\n",
    "        from IPython.display import display\n",
    "        \n",
    "        uploader = FileUpload(accept='.csv', multiple=False)\n",
    "        display(uploader)\n",
    "        \n",
    "        def on_upload_change(change):\n",
    "            if change['type'] == 'change' and change['name'] == 'value':\n",
    "                filename = list(change['new'].keys())[0]\n",
    "                content = change['new'][filename]['content']\n",
    "                with open(filename, 'wb') as f:\n",
    "                    f.write(content)\n",
    "                print(f\"Uploaded file: {filename}\")\n",
    "        \n",
    "        uploader.observe(on_upload_change, names='value')\n",
    "        return uploader\n",
    "\n",
    "# Set up OpenAI API key\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "elif IN_COLAB and userdata.get('OPENAI_API_KEY'):\n",
    "    # Add OPENAI_API_KEY as secret in google colab to skip manual entry\n",
    "    client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
    "else:\n",
    "    client = OpenAI(api_key=getpass(\"Please enter your OpenAI API key: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cache to efficiently store and reuse LLM responses\n",
    "import hashlib\n",
    "from functools import lru_cache\n",
    "\n",
    "# Create a cache dictionary\n",
    "llm_cache = {}\n",
    "\n",
    "# Create a function to generate a unique key for each prompt\n",
    "def generate_cache_key(prompt):\n",
    "    return hashlib.md5(f\"{prompt}\".encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Upload and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# Define flags to control whether to use default files\n",
    "USE_DEFAULT_DATA = True\n",
    "USE_DEFAULT_COMMUNITY = True\n",
    "DATA_FILE_URL = \"https://raw.githubusercontent.com/collect-intel/osccai-simulation/refs/heads/main/data/ccai_polis_data_voters.csv\"\n",
    "COMMUNITY_FILE_URL = \"https://raw.githubusercontent.com/collect-intel/osccai-simulation/refs/heads/main/data/simulation_community_inputs.json\"\n",
    "\n",
    "# Function to curl the CSV from GitHub\n",
    "def curl_github_file(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text if response.status_code == 200 else None\n",
    "\n",
    "# Get actual data to base distributions on\n",
    "if USE_DEFAULT_DATA:\n",
    "    print(\"Attempting to load data from GitHub...\")\n",
    "    csv_content = curl_github_file(DATA_FILE_URL)\n",
    "    if csv_content:\n",
    "        data = pd.read_csv(StringIO(csv_content))\n",
    "    else:\n",
    "        print(\"Failed to load default data. Please upload manually.\")\n",
    "        USE_DEFAULT_DATA = False\n",
    "\n",
    "if not USE_DEFAULT_DATA:\n",
    "    print(\"Please upload your Polis data CSV file.\")\n",
    "    if IN_COLAB:\n",
    "        uploaded = files.upload()\n",
    "        filename = list(uploaded.keys())[0]\n",
    "    else:\n",
    "        uploader = upload_files()\n",
    "        # Wait for the user to upload a file\n",
    "        while not uploader.value:\n",
    "            pass\n",
    "        filename = list(uploader.value.keys())[0]\n",
    "\n",
    "    # Load the uploaded CSV into a DataFrame\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "# Verify that data was loaded\n",
    "if data is not None:\n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(f\"Shape of the data: {data.shape}\")\n",
    "else:\n",
    "    print(\"Failed to load data. Please check your input or try uploading manually.\")\n",
    "\n",
    "# Load community inputs\n",
    "if USE_DEFAULT_COMMUNITY:\n",
    "    print(\"Attempting to load community inputs from GitHub...\")\n",
    "    community_json = curl_github_file(COMMUNITY_FILE_URL)\n",
    "    if community_json:\n",
    "        community_data = json.loads(community_json)[0]  # Assuming we want the first community\n",
    "        print(\"Community inputs loaded successfully.\")\n",
    "    else:\n",
    "        print(\"Failed to load default community inputs. Will prompt for manual input.\")\n",
    "        USE_DEFAULT_COMMUNITY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate additional metrics\n",
    "\n",
    "# Calculate pass votes per participant\n",
    "data['n-pass'] = data['n-votes'] - (data['n-agree'] + data['n-disagree'])\n",
    "\n",
    "# Calculate % agree, % disagree, and % pass per participant\n",
    "data['% agree'] = data['n-agree'] / data['n-votes']\n",
    "data['% disagree'] = data['n-disagree'] / data['n-votes']\n",
    "data['% pass'] = data['n-pass'] / data['n-votes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Simulation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_beta_distribution(data):\n",
    "    \"\"\"Fit a beta distribution to the given data.\"\"\"\n",
    "    data_cleaned = data.clip(lower=0.001, upper=0.999)\n",
    "    a, b, _, _ = beta.fit(data_cleaned, floc=0, fscale=1)\n",
    "    return a, b\n",
    "\n",
    "def fit_negative_binomial(data):\n",
    "    \"\"\"Fit a negative binomial distribution to the given data.\"\"\"\n",
    "    mean = data.mean()\n",
    "    var = data.var()\n",
    "    n = (mean ** 2) / (var - mean) if var > mean else 10\n",
    "    p = mean / (mean + n)\n",
    "    return n, p\n",
    "    \n",
    "def calculate_statements_per_participant(num_participants):\n",
    "    \"\"\"Calculate the number of statements per participant based on actual data.\"\"\"\n",
    "    fattening_factor = 3 # fatten the right tail of the skew\n",
    "    max_statements = 20\n",
    "    # Add 1 to all values in the actual data to shift up any 0's to 1 minimum\n",
    "    n_comments = data['n-comments'] + 1\n",
    "    n, p = fit_negative_binomial(n_comments)\n",
    "    \n",
    "    # Adjust parameters to fatten the tail\n",
    "    adjusted_n = n / fattening_factor\n",
    "    adjusted_p = adjusted_n / (adjusted_n + n_comments.mean())\n",
    "    \n",
    "    # Generate samples from the adjusted negative binomial distribution\n",
    "    samples = np.random.negative_binomial(n=adjusted_n, p=adjusted_p, size=num_participants)\n",
    "    \n",
    "    # Ensure at least 1 statement per participant and cap at max_statements\n",
    "    return [min(max(1, sample), max_statements) for sample in samples]\n",
    "\n",
    "def sample_statements_per_participant(num_participants):\n",
    "    \"\"\"Sample the number of statements per participant directly from actual data.\"\"\"\n",
    "    max_statements = 20\n",
    "    \n",
    "    # Ensure n-comments has at least 1 comment per participant\n",
    "    n_comments = np.maximum(data['n-comments'], 1)\n",
    "    \n",
    "    # Sample with replacement from the actual data\n",
    "    samples = np.random.choice(n_comments, size=num_participants, replace=True)\n",
    "    \n",
    "    # Cap at max_statements\n",
    "    return [min(sample, max_statements) for sample in samples]\n",
    "\n",
    "def calculate_votes_per_participant(num_participants, max_statements):\n",
    "    \"\"\"Calculate the number of votes per participant based on actual data, capped by max_statements.\"\"\"\n",
    "    n_votes = data['n-votes']\n",
    "    n, p = fit_negative_binomial(n_votes)\n",
    "    return [min(max(np.random.negative_binomial(n, p), 1), max_statements) for _ in range(num_participants)]\n",
    "\n",
    "def calculate_vote_distribution_per_participant(num_participants):\n",
    "    \"\"\"Calculate the vote distribution per participant based on actual data.\"\"\"\n",
    "    agree_a, agree_b = fit_beta_distribution(data['% agree'])\n",
    "    disagree_a, disagree_b = fit_beta_distribution(data['% disagree'])\n",
    "\n",
    "    distributions = []\n",
    "    for _ in range(num_participants):\n",
    "        agree = np.random.beta(agree_a, agree_b)\n",
    "        disagree = np.random.beta(disagree_a, disagree_b)\n",
    "        pass_prob = max(0, 1 - (agree + disagree))\n",
    "        total = agree + disagree + pass_prob\n",
    "        distributions.append([round(agree/total, 3), round(disagree/total, 3), round(pass_prob/total, 3)])\n",
    "\n",
    "    return distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Define OpenAI Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=10), stop=stop_after_attempt(3))\n",
    "def get_openai_completion(prompt):\n",
    "    \"\"\"Sends a prompt to the OpenAI API and returns the completion, using cache if available.\"\"\"\n",
    "    cache_key = generate_cache_key(prompt)\n",
    "\n",
    "    if cache_key in llm_cache:\n",
    "        print(\"Using cached response\")\n",
    "        return llm_cache[cache_key]\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant that outputs JSON-formatted data.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        completion = response.choices[0].message.content\n",
    "\n",
    "        # Cache the response\n",
    "        llm_cache[cache_key] = completion\n",
    "        print(\"Received response from OpenAI\")\n",
    "        return completion\n",
    "    except Exception as e:\n",
    "        if \"RateLimitError\" in str(type(e)):\n",
    "            retry_after = int(e.headers.get(\"Retry-After\", 10)) if getattr(e, 'headers', None) else 10\n",
    "            print(f\"Rate limit exceeded. Retrying after {retry_after} seconds.\")\n",
    "            time.sleep(int(retry_after))\n",
    "            raise\n",
    "        print(f\"Error during OpenAI API call: {e}\")\n",
    "        raise  # Re-raise the exception to stop execution\n",
    "\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from asyncio import TimeoutError\n",
    "\n",
    "# Apply nest_asyncio to run with an event loop already running in the notebook\n",
    "nest_asyncio.apply()\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "async def get_openai_completion_async(prompt):\n",
    "    \"\"\"Asynchronous version of get_openai_completion with caching.\"\"\"\n",
    "    print(f\"Sending prompt to OpenAI asynchronously (length: {len(prompt)})\")\n",
    "    loop = asyncio.get_running_loop()\n",
    "    try:\n",
    "        with ThreadPoolExecutor() as pool:\n",
    "            return await asyncio.wait_for(\n",
    "                loop.run_in_executor(pool, get_openai_completion, prompt),\n",
    "                timeout=120  # 120 seconds timeout\n",
    "            )\n",
    "    except TimeoutError:\n",
    "        print(\"API call timed out\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during API call: {e}\")\n",
    "        return None\n",
    "\n",
    "# returns json string\n",
    "async def get_openai_completion_chunked(prompt, data, chunk_size=10):\n",
    "    print(f\"Starting chunked completion with {len(data)} items, chunk size {chunk_size}\")\n",
    "    full_response = {\"data\": []}\n",
    "\n",
    "    chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "    print(f\"Prepared {len(chunks)} chunks for processing\")\n",
    "\n",
    "    async def process_chunk(chunk_index, chunk):\n",
    "        chunk_prompt = f\"{prompt}\\nProcess the following chunk:\\n{json.dumps(chunk)}\"\n",
    "        try:\n",
    "            response = await get_openai_completion_async(chunk_prompt)\n",
    "            print(f\"Received response for chunk {chunk_index + 1}\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {chunk_index + 1}: {e}\")\n",
    "            return None\n",
    "\n",
    "    async_responses = await asyncio.gather(*[process_chunk(i, chunk) for i, chunk in enumerate(chunks)])\n",
    "\n",
    "    for chunk_response in async_responses:\n",
    "        if chunk_response:\n",
    "            try:\n",
    "                chunk_data = json.loads(chunk_response)\n",
    "                full_response[\"data\"].extend(chunk_data[\"data\"])\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON from chunk response: {e}\")\n",
    "\n",
    "    print(f\"Processed {len(async_responses)} chunks\")\n",
    "\n",
    "    return full_response\n",
    "\n",
    "# Example usage of get_openai_completion_chunked\n",
    "async def process_data_async():\n",
    "    # ... other code ...\n",
    "    chunked_response = await get_openai_completion_chunked(prompt, data, chunk_size=10)\n",
    "    # ... process chunked_response ...\n",
    "# In the main execution:\n",
    "# asyncio.run(process_data_async())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. User Input Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Community Information Collection ###\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'USE_DEFAULT_COMMUNITY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### Community Information Collection ###\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mUSE_DEFAULT_COMMUNITY\u001b[49m:\n\u001b[1;32m      3\u001b[0m     community_name \u001b[38;5;241m=\u001b[39m community_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommunity_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m     community_description \u001b[38;5;241m=\u001b[39m community_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommunity_description\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'USE_DEFAULT_COMMUNITY' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"### Community Information Collection ###\")\n",
    "if USE_DEFAULT_COMMUNITY:\n",
    "    community_name = community_data['community_name']\n",
    "    community_description = community_data['community_description']\n",
    "    community_goals = community_data['community_goals_for_ai_model']\n",
    "    print(f\"Using default community: {community_name}\")\n",
    "else:\n",
    "    community_name = input(\"Enter the community name: \").strip()\n",
    "    community_description = input(\"Enter the community description: \").strip()\n",
    "    community_goals = input(\"Enter the community goals for the AI model: \").strip()\n",
    "\n",
    "# Consolidate inputs into a single community description\n",
    "full_community_description = f\"{community_description}\\nGoals: {community_goals}\"\n",
    "\n",
    "print(\"\\n### Simulation Parameters Collection ###\")\n",
    "if USE_DEFAULT_COMMUNITY:\n",
    "    num_subgroups = 2\n",
    "    num_participants = 20\n",
    "    print(f\"Using default values: {num_subgroups} subgroups, {num_participants} participants\")\n",
    "else:\n",
    "    while True:\n",
    "        try:\n",
    "            num_subgroups = int(input(\"Enter the number of subgroups (G) [default: 2]: \") or \"2\")\n",
    "            if num_subgroups <= 0:\n",
    "                raise ValueError(\"Number of subgroups must be positive.\")\n",
    "            break\n",
    "        except ValueError as e:\n",
    "            print(f\"Invalid input: {e}\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            num_participants = int(input(\"Enter the total number of participants (P) [default: 20]: \") or \"20\")\n",
    "            if num_participants <= 0:\n",
    "                raise ValueError(\"Number of participants must be positive.\")\n",
    "            break\n",
    "        except ValueError as e:\n",
    "            print(f\"Invalid input: {e}\")\n",
    "\n",
    "statement_format = input(\"Enter the statement format (default: 'The best response is one that...'): \").strip()\n",
    "if not statement_format:\n",
    "    statement_format = \"The best response is one that...\"\n",
    "\n",
    "# upload a saved llm_cache file to resume a previous simulation\n",
    "use_saved_cache = input(\"Do you want to use a saved LLM cache? (y/n): \").lower().strip() == 'y'\n",
    "if use_saved_cache:\n",
    "    if IN_COLAB:\n",
    "        uploaded = files.upload()\n",
    "        cache_file = list(uploaded.keys())[0]\n",
    "    else:\n",
    "        uploader = upload_files()\n",
    "        while not uploader.value:\n",
    "            pass\n",
    "        cache_file = list(uploader.value.keys())[0]\n",
    "    \n",
    "    llm_cache.update(load_llm_cache(cache_file))\n",
    "    print(f\"Loaded {len(llm_cache)} items into LLM cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Simulation Setup Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_participants_per_group(num_subgroups, num_participants):\n",
    "    \"\"\"Distribute participants among groups with variability but no extreme disparities.\"\"\"\n",
    "    base = num_participants // num_subgroups\n",
    "    remainder = num_participants % num_subgroups\n",
    "    participants_per_group = [base] * num_subgroups\n",
    "    for i in range(remainder):\n",
    "        participants_per_group[i] += 1\n",
    "    # Introduce slight variability\n",
    "    for i in range(len(participants_per_group)):\n",
    "        variation = np.random.randint(-base//10, base//10+1)\n",
    "        participants_per_group[i] = max(1, participants_per_group[i] + variation)\n",
    "    total_participants = sum(participants_per_group)\n",
    "    # Adjust if total participants changed due to variability\n",
    "    if total_participants != num_participants:\n",
    "        difference = num_participants - total_participants\n",
    "        for i in range(abs(difference)):\n",
    "            index = i % num_subgroups\n",
    "            if difference > 0:\n",
    "                participants_per_group[index] += 1\n",
    "            else:\n",
    "                participants_per_group[index] = max(1, participants_per_group[index] - 1)\n",
    "    return participants_per_group\n",
    "\n",
    "participants_per_group = calculate_participants_per_group(num_subgroups, num_participants)\n",
    "print(f\"\\nParticipants per group: {participants_per_group}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LLM Interaction for Group and Participant Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate subgroups\n",
    "subgroup_generation_prompt = f\"\"\"\n",
    "You are tasked with creating realistic subgroups for a community simulation. Use the following information to generate detailed descriptions:\n",
    "\n",
    "Community Description: {full_community_description}\n",
    "Number of Subgroups: {num_subgroups}\n",
    "\n",
    "For each subgroup:\n",
    "1. Provide a 2-sentence description of the subgroup. This description must start with the words: \"This group consists of individuals who...\"\n",
    "\n",
    "Ensure descriptions are diverse and realistic within the context of the community.\n",
    "\n",
    "Return your response in the following JSON format:\n",
    "{{\"subgroups\": [\n",
    "    {{\"description\": \"This group consists of individuals who <rest of description>\"}},\n",
    "    ...]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "subgroup_generation_response = get_openai_completion(subgroup_generation_prompt)\n",
    "\n",
    "try:\n",
    "    subgroup_data = json.loads(subgroup_generation_response)\n",
    "    subgroups = subgroup_data['subgroups']\n",
    "    print(\"\\nGenerated Subgroups:\")\n",
    "    print(json.dumps(subgroups, indent=4))\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error parsing JSON: {e}\")\n",
    "    subgroups = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_subgroup_data = [\n",
    "    {\n",
    "        \"subgroup_index\": i,\n",
    "        \"num_participants\": num_participants,\n",
    "        \"description\": subgroup['description']\n",
    "    }\n",
    "    for i, (subgroup, num_participants) in enumerate(zip(subgroups, participants_per_group))\n",
    "]\n",
    "\n",
    "async def generate_participants():\n",
    "    participant_generation_prompt = \"\"\"\n",
    "    Generate unique, 1-sentence descriptions of individuals who could belong to the following subgroups:\n",
    "    \n",
    "    For each subgroup in the input data, generate the specified number of participant descriptions.\n",
    "    Ensure descriptions are diverse and realistic within the context of the community.\n",
    "    \n",
    "    The description of each participant should take this form: \"A <demographic description> who is <extended description of situation, personality, or goals>.\"\n",
    "    Do not include the name of the participant.\n",
    "\n",
    "    Return your response in the following JSON format:\n",
    "    {\"data\": [{ \"subgroup_index\": 0, \"participants\": [\"Participant 1 description\", \"Participant 2 description\", ...] },\n",
    "              { \"subgroup_index\": 1, \"participants\": [...] },\n",
    "              ...]}\n",
    "    \"\"\"\n",
    "\n",
    "    chunked_response = await get_openai_completion_chunked(\n",
    "        participant_generation_prompt, \n",
    "        indexed_subgroup_data, \n",
    "        chunk_size=1  # Adjust this value as needed\n",
    "    )\n",
    "\n",
    "    return chunked_response\n",
    "\n",
    "# Run the async function to generate participants\n",
    "try:\n",
    "    # This will work in both Jupyter and Colab\n",
    "    loop = asyncio.get_event_loop()\n",
    "    all_participants = loop.run_until_complete(generate_participants())\n",
    "except RuntimeError:\n",
    "    # Fallback for environments where get_event_loop() might fail\n",
    "    all_participants = asyncio.run(generate_participants())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine subgroups by index from all_subgroups_data and participants from all_participants into group_data\n",
    "group_data = {\n",
    "    \"subgroups\": [\n",
    "        {\n",
    "            \"description\": subgroup_data['description'],\n",
    "            \"participants\": next(item for item in all_participants['data'] if item[\"subgroup_index\"] == subgroup_data[\"subgroup_index\"])[\"participants\"]\n",
    "        }\n",
    "        for subgroup_data in indexed_subgroup_data\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nGenerated Subgroups and Participants:\")\n",
    "print(json.dumps(group_data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LLM Interaction for Statement Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of participants\n",
    "total_participants = sum(len(subgroup['participants']) for subgroup in group_data['subgroups'])\n",
    "\n",
    "# Sample statements per participant - Ensure the number of samples matches the total number of participants\n",
    "statements_per_participant = sample_statements_per_participant(total_participants)\n",
    "\n",
    "# Create an iterator from statements_per_participant\n",
    "statements_iter = iter(statements_per_participant)\n",
    "\n",
    "# Prepare the data structure for chunked processing\n",
    "chunkable_data = []\n",
    "participant_id_counter = 1  # Initialize a counter for unique participant IDs\n",
    "\n",
    "for subgroup_index, subgroup in enumerate(group_data['subgroups']):\n",
    "    for participant in subgroup['participants']:\n",
    "        chunkable_data.append({\n",
    "            'participant_id': participant_id_counter,\n",
    "            'subgroup_index': int(subgroup_index),\n",
    "            'num_statements': int(next(statements_iter)),\n",
    "            'participant': participant\n",
    "        })\n",
    "        participant_id_counter += 1  # Increment the counter for the next participant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that all statements_per_participant values were used\n",
    "if any(True for _ in statements_iter):\n",
    "    print(\"Warning: Not all statements_per_participant values were used.\")\n",
    "\n",
    "# Prepare subgroup descriptions\n",
    "subgroup_descriptions = [\n",
    "    f\"Subgroup {i}: {subgroup['description']}\"\n",
    "    for i, subgroup in enumerate(group_data['subgroups'])\n",
    "]\n",
    "\n",
    "statement_generation_prompt = f\"\"\"\n",
    "Generate statements for a community AI model alignment survey. Use the following information:\n",
    "\n",
    "Community Description: {full_community_description}\n",
    "Statement Format: \"{statement_format}\"\n",
    "\n",
    "Subgroup Descriptions:\n",
    "{json.dumps(subgroup_descriptions, indent=2)}\n",
    "\n",
    "For each participant in the input data, generate the specified number of statements that align with their subgroup and individual characteristics. Ensure statements are diverse and relevant to the community's goals.\n",
    "\n",
    "The input data format is:\n",
    "[participant_id, subgroup_index, number_of_statements_to_generate, participant_description]\n",
    "\n",
    "Return your response in the following JSON format:\n",
    "{{\n",
    "    \"data\": [\n",
    "        {{\n",
    "            \"participant_id\": \"<participant_id>\",\n",
    "            \"statements\": [\"Statement 1\", \"Statement 2\", ...]\n",
    "        }},\n",
    "        ...\n",
    "    ]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "async def generate_statements():\n",
    "    chunked_response = await get_openai_completion_chunked(\n",
    "        statement_generation_prompt, \n",
    "        chunkable_data, \n",
    "        chunk_size=10  # Adjust this value as needed\n",
    "    )\n",
    "    return chunked_response\n",
    "\n",
    "# Run the async function to generate statements\n",
    "try:\n",
    "    # This will work in both Jupyter and Colab\n",
    "    loop = asyncio.get_event_loop()\n",
    "    all_statements = loop.run_until_complete(generate_statements())\n",
    "except RuntimeError:\n",
    "    # Fallback for environments where get_event_loop() might fail\n",
    "    all_statements = asyncio.run(generate_statements())\n",
    "\n",
    "# convert list of dict objects: [{\"participant_id\":<participant_id>, \"statements\":[<statements>]}] to a dict with keys: {<participant_id>:[<statements>],}\n",
    "participant_statements = {item['participant_id']: item['statements'] for item in all_statements['data']}\n",
    "\n",
    "try:\n",
    "    print(\"\\nGenerated Participant Statements:\")\n",
    "    print(json.dumps(participant_statements, indent=4))\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error parsing JSON: {e}\")\n",
    "    participant_statements = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the number of statements per participant is consistent\n",
    "assert statements_per_participant == [len(s) for s in participant_statements.values()] == [len(i['statements']) for i in  all_statements['data']] == [i['num_statements'] for i in chunkable_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. LLM Interaction for Vote Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. LLM Interaction for Vote Simulation\n",
    "\n",
    "print(\"Starting vote simulation process...\")\n",
    "\n",
    "# Prepare numbered statements list\n",
    "statements_list = []\n",
    "statement_id = 1\n",
    "for participant_id, statements in participant_statements.items():\n",
    "    for stmt in statements:\n",
    "        statements_list.append({'statement_id': statement_id, 'text': stmt})\n",
    "        statement_id += 1\n",
    "\n",
    "print(f\"Prepared {len(statements_list)} statements for {len(participant_statements)} participants\")\n",
    "\n",
    "numbered_statements_list = json.dumps(statements_list, indent=4)\n",
    "\n",
    "# Prepare participants and assignments data\n",
    "max_statements = len(statements_list)\n",
    "votes_per_participant = calculate_votes_per_participant(len(participant_statements), max_statements)\n",
    "vote_distributions = calculate_vote_distribution_per_participant(len(participant_statements))\n",
    "\n",
    "print(f\"Calculated votes per participant and vote distributions\")\n",
    "\n",
    "chunkable_data = []\n",
    "for i, (participant_id, statements) in enumerate(participant_statements.items()):\n",
    "    num_votes = votes_per_participant[i]\n",
    "    available_statements = [s['statement_id'] for s in statements_list]\n",
    "    assigned_statements = random.sample(available_statements, min(num_votes, len(available_statements)))\n",
    "    chunkable_data.append({\n",
    "        'participant_id': participant_id,\n",
    "        'statements_assigned': assigned_statements,\n",
    "        'vote_distribution': dict(zip(['agree', 'disagree', 'pass'], vote_distributions[i]))\n",
    "    })\n",
    "\n",
    "print(f\"Prepared chunkable data for {len(chunkable_data)} participants\")\n",
    "\n",
    "vote_simulation_prompt = f\"\"\"\n",
    "Simulate voting patterns for a community AI model alignment survey. Use the following information:\n",
    "\n",
    "Community Description: {full_community_description}\n",
    "Statements:\n",
    "{numbered_statements_list}\n",
    "\n",
    "For each participant in the input data, determine how they would likely vote on their assigned statements. Use the following voting options:\n",
    "1 = Agree\n",
    "-1 = Disagree\n",
    "0 = Pass\n",
    "\n",
    "Ensure that each participant's voting pattern closely matches their target vote distribution.\n",
    "\n",
    "The input data format is:\n",
    "[participant_id, statements_assigned, vote_distribution]\n",
    "\n",
    "Return your response in the following JSON format:\n",
    "{{\n",
    "    \"data\": [\n",
    "        {{\n",
    "            \"participant_id\": \"Unique identifier\",\n",
    "            \"votes\": {{\"statement_id\": vote, ...}}\n",
    "        }},\n",
    "        ...\n",
    "    ]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Prepared vote simulation prompt\")\n",
    "\n",
    "async def simulate_votes():\n",
    "    print(\"Starting vote simulation...\")\n",
    "    chunked_response = await get_openai_completion_chunked(\n",
    "        vote_simulation_prompt,\n",
    "        chunkable_data,\n",
    "        chunk_size=5  # Adjust this value as needed\n",
    "    )\n",
    "    print(\"Completed vote simulation\")\n",
    "    return chunked_response\n",
    "\n",
    "# Run the async function to simulate votes\n",
    "print(\"Initiating async vote simulation...\")\n",
    "try:\n",
    "    loop = asyncio.get_event_loop()\n",
    "    vote_simulation_response = loop.run_until_complete(simulate_votes())\n",
    "    print(\"Async vote simulation completed\")\n",
    "except RuntimeError:\n",
    "    print(\"RuntimeError occurred, falling back to asyncio.run()\")\n",
    "    vote_simulation_response = asyncio.run(simulate_votes())\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    vote_simulation_response = None\n",
    "\n",
    "if vote_simulation_response:\n",
    "    try:\n",
    "        vote_data = vote_simulation_response['data']\n",
    "        # make sure vote statement_ids are ints\n",
    "        participant_votes = {item['participant_id']: {int(stmt_id): vote for stmt_id, vote in item['votes'].items()} for item in vote_data}\n",
    "        # participant_votes = { <participant_id>: {<statement_id>: <vote>, ...}, ... }\n",
    "        print(\"\\nSimulated Votes:\")\n",
    "        print(json.dumps(participant_votes, indent=4))\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        print(f\"Error processing vote data: {e}\")\n",
    "        participant_votes = {}\n",
    "else:\n",
    "    print(\"No vote simulation response received\")\n",
    "    participant_votes = {}\n",
    "\n",
    "print(\"Vote simulation process completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Vote Matrix Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty matrix (rows = participants, columns = statements)\n",
    "participant_ids = list(participant_votes.keys())\n",
    "statement_ids = sorted(set(stmt_id for votes in participant_votes.values() for stmt_id in votes))\n",
    "\n",
    "vote_matrix = pd.DataFrame(index=participant_ids, columns=statement_ids)\n",
    "\n",
    "# Populate matrix with votes\n",
    "for participant_id, votes in participant_votes.items():\n",
    "    for stmt_id, vote_value in votes.items():\n",
    "        vote_matrix.at[participant_id, stmt_id] = vote_value\n",
    "\n",
    "# Convert data types\n",
    "vote_matrix = vote_matrix.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Results Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Create a custom colormap\n",
    "cmap = mcolors.ListedColormap(['red', 'gray', 'green'])\n",
    "bounds = [-1.5, -0.5, 0.5, 1.5]\n",
    "norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(vote_matrix.fillna(0), aspect='auto', cmap=cmap, norm=norm, interpolation='none')\n",
    "\n",
    "# Create a custom colorbar\n",
    "cbar = plt.colorbar(ticks=[-1, 0, 1])\n",
    "cbar.set_ticklabels(['Disagree', 'Pass', 'Agree'])\n",
    "cbar.set_label('Vote')\n",
    "\n",
    "plt.xlabel('Statements')\n",
    "plt.ylabel('Participants')\n",
    "plt.title('Vote Matrix Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display vote distribution statistics\n",
    "vote_counts = vote_matrix.stack().value_counts()\n",
    "print(\"\\n### Vote Distribution Statistics ###\")\n",
    "print(vote_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert vote_matrix to a list of lists\n",
    "vote_matrix_list = vote_matrix.values.tolist()\n",
    "\n",
    "# Replace NaN values with None\n",
    "vote_matrix_list = [[None if pd.isna(value) else int(value) for value in row] for row in vote_matrix_list]\n",
    "\n",
    "# Print the result\n",
    "print(\"Vote matrix as a list of lists:\")\n",
    "print(\"[\")\n",
    "for row in vote_matrix_list:\n",
    "    print(f\"    {row},\")\n",
    "print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11. Data Export and LLM Cache Management\n",
    "\n",
    "import json\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Create a dictionary mapping statement_id to statement text\n",
    "statement_dict = {item['statement_id']: item['text'] for item in statements_list}\n",
    "\n",
    "# Create a reverse mapping of statement text to id\n",
    "text_to_id = {text: id for id, text in statement_dict.items()}\n",
    "\n",
    "# Prepare the data structure\n",
    "export_data = {\n",
    "    \"community_info\": {\n",
    "        \"name\": community_name,\n",
    "        \"description\": community_description,\n",
    "        \"goals\": community_goals\n",
    "    },\n",
    "    \"user_inputs\": {\n",
    "        \"num_subgroups\": num_subgroups,\n",
    "        \"num_participants\": num_participants\n",
    "    },\n",
    "    \"statement_format\": statement_format,\n",
    "    \"statements\": [\n",
    "        {\"id\": stmt_id, \"text\": text} for stmt_id, text in statement_dict.items()\n",
    "    ],\n",
    "    \"subgroups\": [\n",
    "        {\n",
    "            \"index\": i,\n",
    "            \"description\": subgroup[\"description\"],\n",
    "            \"participants\": [\n",
    "                {\n",
    "                    \"id\": participant_id,\n",
    "                    \"description\": participant,\n",
    "                    \"statements\": [\n",
    "                        {\"id\": text_to_id[stmt], \"text\": stmt}\n",
    "                        for stmt in participant_statements.get(int(participant_id), [])\n",
    "                        if stmt in text_to_id\n",
    "                    ],\n",
    "                    \"votes\": {int(k): v for k, v in participant_votes.get(int(participant_id), {}).items()}\n",
    "                }\n",
    "                for participant_id, participant in enumerate(subgroup[\"participants\"], start=1)\n",
    "            ]\n",
    "        }\n",
    "        for i, subgroup in enumerate(group_data[\"subgroups\"])\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Export the data to a JSON file\n",
    "with open('simulation_data.json', 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "print(\"Simulation data exported to 'simulation_data.json'\")\n",
    "\n",
    "# Export the LLM cache\n",
    "with open('llm_cache.json', 'w') as f:\n",
    "    json.dump(llm_cache, f, indent=2)\n",
    "\n",
    "print(\"LLM cache exported to 'llm_cache.json'\")\n",
    "\n",
    "# Provide download links for Colab or local\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    files.download('simulation_data.json')\n",
    "    files.download('llm_cache.json')\n",
    "else:\n",
    "    from IPython.display import FileLink\n",
    "    display(FileLink('simulation_data.json'))\n",
    "    display(FileLink('llm_cache.json'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
